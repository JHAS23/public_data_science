{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import regex\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import missingno as msno\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "#sklearn.model_selection\n",
    "porter_stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_ind = pd.read_csv('train/automobile_industry.csv')\n",
    "comp_ind = pd.read_csv('train/computer_industry.csv')\n",
    "health_ind = pd.read_csv('train/health_industry.csv')\n",
    "manu_ind = pd.read_csv('train/manufacturing_industry.csv')\n",
    "power_ind = pd.read_csv('train/power_industry.csv')\n",
    "\n",
    "test = pd.read_csv('test/test.csv')\n",
    "#Add Industry Identifier\n",
    "auto_ind['Industry']='Automobile'\n",
    "comp_ind['Industry']='Computer'\n",
    "health_ind['Industry']='Health'\n",
    "manu_ind['Industry']='Manufacturing'\n",
    "power_ind['Industry']='Power'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(auto_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Product Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = os.listdir('train/product_info')\n",
    "strtxt = \".txt\"\n",
    "all_product_info = pd.DataFrame()\n",
    "for txtfile in arr:\n",
    "    if txtfile.__contains__(strtxt):\n",
    "        file_add = str('train/product_info/')+str(txtfile)\n",
    "        fileObject = open(file_add, \"r\", encoding=\"unicode_escape\")\n",
    "        data = fileObject.read()\n",
    "        product_info_data = pd.DataFrame([[txtfile.replace(strtxt,'') ,data]],columns = ['ID','product_info'])\n",
    "        all_product_info = all_product_info.append(product_info_data)\n",
    "\n",
    "# Merge Product Info with Training Data\n",
    "auto_ind = auto_ind.merge(all_product_info, on='ID', how='left')\n",
    "comp_ind = comp_ind.merge(all_product_info, on='ID', how='left')\n",
    "health_ind = health_ind.merge(all_product_info, on='ID', how='left')\n",
    "manu_ind = manu_ind.merge(all_product_info, on='ID', how='left')\n",
    "power_ind = power_ind.merge(all_product_info, on='ID', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Nulls and Missing Values and define Dependent & Independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_columns = ['ID']\n",
    "x_columns = ['Company_background']\n",
    "y_columns = ['Industry','Type','Product']\n",
    "def fix_type(df):\n",
    "    df = df.fillna('')\n",
    "    df = df.replace('?','')\n",
    "    df['Type'] = df['Type'].str.replace('vehicle','Vehicles')\n",
    "    y_combs = df[y_columns].drop_duplicates()\n",
    "    y_combs['y_identifier']=np.arange(0,len(y_combs))\n",
    "    df = df.merge(y_combs, on = y_columns,how='inner')\n",
    "    y_dist = pd.DataFrame({'row_count': df.groupby(y_columns)['Company_background'].count()}).reset_index()\n",
    "    imputed_type_combination = y_dist.groupby('Product')['row_count'].max().reset_index()\n",
    "    imputed_type_combination = imputed_type_combination.merge(y_dist, on = ['Product','row_count'],how='inner').reset_index(drop = True)\n",
    "    imputed_type_combination = imputed_type_combination.reindex(columns = ['Industry','Type','Product','row_count'])\n",
    "    final_y_combinations = y_combs.drop('Type',axis =1).merge(imputed_type_combination,on = ['Industry','Product'],how = 'left')\n",
    "    final_y_combinations = final_y_combinations.reindex(columns = ['Industry','Type','Product','y_identifier'])\n",
    "    df = df.drop(y_columns,axis=1)\n",
    "    df = df.merge(final_y_combinations, on ='y_identifier',how='inner')\n",
    "    return df\n",
    "\n",
    "auto_ind_imp_type = fix_type(auto_ind)\n",
    "comp_ind_imp_type = fix_type(comp_ind)\n",
    "health_ind_imp_type = fix_type(health_ind)\n",
    "manu_ind_imp_type = fix_type(manu_ind)\n",
    "power_ind_imp_type = fix_type(power_ind)\n",
    "input_df = pd.concat([auto_ind_imp_type,comp_ind_imp_type,health_ind_imp_type,manu_ind_imp_type,power_ind_imp_type])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NlP Processing for Text Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#define NLP Functions\n",
    "def cleantext(val):\n",
    "    val = str(val).replace(r'[^\\x00-\\x7F]+', '')\n",
    "    if len(val.split()) >=2:\n",
    "        val = val.lower()\n",
    "        tk = word_tokenize(val)\n",
    "        #tk = [porter.stem(word) for word in tk]\n",
    "        #tk = [lemmatizer.lemmatize(word) for word in tk]\n",
    "        #tk = [w.translate(table) for w in tk]\n",
    "        tk = [word for word in tk if word.isalpha()]\n",
    "        tk = [w for w in tk if not w in stop_words]\n",
    "        return ' '.join(tk)\n",
    "    else:\n",
    "        return 'Not Avaiable'\n",
    "\n",
    "def preprocess_dataframe(input_df,x_columns,y_columns=None):\n",
    "\n",
    "    if(y_columns==None):\n",
    "        df = input_df.drop(x_columns,axis=1) \n",
    "    else:\n",
    "        df = input_df.reindex(columns = y_columns)\n",
    "    \n",
    "    df['text'] = input_df[x_columns].apply(lambda x: ' '.join(x.map(str)), axis=1)\n",
    "    df['text']= df['text'].apply(lambda x: x.lower())\n",
    "    df['clean'] = df['text'].apply(cleantext)\n",
    "    df = df.dropna(axis=0, subset=['clean'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call PreProcess & NLP Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_processed = preprocess_dataframe(auto_ind_imp_type,x_columns,y_columns)\n",
    "comp_processed = preprocess_dataframe(comp_ind_imp_type,x_columns,y_columns)\n",
    "health_processed = preprocess_dataframe(health_ind_imp_type,x_columns,y_columns)\n",
    "manu_processed = preprocess_dataframe(manu_ind_imp_type,x_columns,y_columns)\n",
    "power_processed = preprocess_dataframe(power_ind_imp_type,x_columns,y_columns)\n",
    "input_df_processed = pd.concat([auto_processed,comp_processed,health_processed,manu_processed,power_processed])\n",
    "test_processed - preprocess_dataframe(test,x_columns,y_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_word_industry_feature_mapp(df,var,n_range):\n",
    "    vectorizer = TfidfVectorizer(min_df=2,ngram_range = (1,n_range),lowercase = True) #ngram_range=(1,2)\n",
    "    distinct_vars = list(df[var].drop_duplicates())\n",
    "    j=0\n",
    "    \n",
    "    for i in distinct_vars:\n",
    "        df_name = 'df'+'_'+i\n",
    "        new_col_name = str(var)+'_'+str(i)\n",
    "        globals()[df_name] = df[df[var]==i]\n",
    "        vectorizer.fit(globals()[df_name]['clean'].apply(lambda x: np.str_(x)))\n",
    "        globals()[df_name+'_list'] = set(vectorizer.get_feature_names())\n",
    "        globals()[df_name+'_list_df'] = pd.DataFrame(globals()[df_name+'_list'],columns = ['words'])\n",
    "        globals()[df_name+'_list_df'][new_col_name] = i\n",
    "        if(j==0):\n",
    "            distinct_words =  globals()[df_name+'_list']\n",
    "        else:\n",
    "            distinct_words = distinct_words ^ globals()[df_name+'_list']  \n",
    "        j=j+1\n",
    "    \n",
    "    distinct_words_df = pd.DataFrame(distinct_words,columns = ['words'])\n",
    "    for i in distinct_vars:\n",
    "        df_name = 'df'+'_'+i\n",
    "        distinct_words_df = distinct_words_df.merge(globals()[df_name+'_list_df'],on = 'words',how= 'left')\n",
    "    \n",
    "    \n",
    "    distinct_words_df['null_count'] = distinct_words_df.isnull().sum(axis =1)\n",
    "    if (var == 'Industry'):\n",
    "        distinct_words_df = distinct_words_df[distinct_words_df['null_count']==4]\n",
    "        distinct_words_df['Feature'] = np.nan\n",
    "        distinct_words_df['Feature'] = distinct_words_df['Feature'].fillna(distinct_words_df.iloc[:,1]).fillna(distinct_words_df.iloc[:,2]).fillna(distinct_words_df.iloc[:,3]).fillna(distinct_words_df.iloc[:,4]).fillna(distinct_words_df.iloc[:,5])\n",
    "        distinct_words_df = distinct_words_df.reindex(columns = ['words','Feature']) \n",
    "    elif(var == 'Type'):\n",
    "        distinct_words_df = distinct_words_df[distinct_words_df['null_count']==11]\n",
    "        distinct_words_df['Feature'] = np.nan\n",
    "        distinct_words_df['Feature'] = distinct_words_df['Feature'].fillna(distinct_words_df.iloc[:,1]).fillna(distinct_words_df.iloc[:,2]).fillna(distinct_words_df.iloc[:,3]).fillna(distinct_words_df.iloc[:,4]).fillna(distinct_words_df.iloc[:,5]).fillna(distinct_words_df.iloc[:,6]).fillna(distinct_words_df.iloc[:,7]).fillna(distinct_words_df.iloc[:,8]).fillna(distinct_words_df.iloc[:,9]).fillna(distinct_words_df.iloc[:,10]).fillna(distinct_words_df.iloc[:,11]).fillna(distinct_words_df.iloc[:,12])\n",
    "        distinct_words_df = distinct_words_df.reindex(columns = ['words','Feature']) \n",
    "    elif(var == 'Product'):\n",
    "        distinct_words_df = distinct_words_df[distinct_words_df['null_count']==24]\n",
    "        distinct_words_df['Feature'] = np.nan\n",
    "        distinct_words_df['Feature'] = distinct_words_df['Feature'].fillna(distinct_words_df.iloc[:,1]).fillna(distinct_words_df.iloc[:,2]).fillna(distinct_words_df.iloc[:,3]).fillna(distinct_words_df.iloc[:,4]).fillna(distinct_words_df.iloc[:,5]).fillna(distinct_words_df.iloc[:,6]).fillna(distinct_words_df.iloc[:,7]).fillna(distinct_words_df.iloc[:,8]).fillna(distinct_words_df.iloc[:,9]).fillna(distinct_words_df.iloc[:,10]).fillna(distinct_words_df.iloc[:,11]).fillna(distinct_words_df.iloc[:,12]).fillna(distinct_words_df.iloc[:,13]).fillna(distinct_words_df.iloc[:,14]).fillna(distinct_words_df.iloc[:,15]).fillna(distinct_words_df.iloc[:,16]).fillna(distinct_words_df.iloc[:,17]).fillna(distinct_words_df.iloc[:,18]).fillna(distinct_words_df.iloc[:,19]).fillna(distinct_words_df.iloc[:,20]).fillna(distinct_words_df.iloc[:,21]).fillna(distinct_words_df.iloc[:,22]).fillna(distinct_words_df.iloc[:,23]).fillna(distinct_words_df.iloc[:,24]).fillna(distinct_words_df.iloc[:,25])\n",
    "        distinct_words_df = distinct_words_df.reindex(columns = ['words','Feature']) \n",
    "\n",
    "    return distinct_words_df\n",
    "\n",
    "def create_vocab(df,n_range):\n",
    "    industry_word_map = create_word_industry_feature_mapp(input_df_processed,'Industry',n_range)\n",
    "    #type_word_map = create_word_industry_feature_mapp(input_df_processed,'Type',n_range)\n",
    "    #product_word_map = create_word_industry_feature_mapp(input_df_processed,'Product',n_range)\n",
    "    #nlp_vocab = pd.concat([industry_word_map,type_word_map,product_word_map])\n",
    "    return industry_word_map\n",
    "#vocab_df = create_vocab(input_df_processed,1)\n",
    "#vocab = vocab_df.applymap(str.strip).applymap(str.lower).set_index('words').Feature\n",
    "#vocab_dict = vocab.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(vectorizer, tfidf_result):\n",
    "    scores = zip(vectorizer.get_feature_names(),\n",
    "                 np.asarray(tfidf_result.sum(axis=0)).ravel())\n",
    "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    return sorted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(gdf):\n",
    "    top_words= pd.DataFrame()\n",
    "    top_gdf = pd.DataFrame()\n",
    "    \n",
    "    for ind in gdf.Industry.unique():\n",
    "        temp = gdf[gdf['Industry']==ind]\n",
    "        for typ in temp.Type.unique():\n",
    "            temp = temp[temp['Type']==typ]\n",
    "            for prd in temp.Product.unique():\n",
    "                temp = temp[temp['Product']==prd]\n",
    "    \n",
    "                try:\n",
    "                    tfIdfVectorizer=TfidfVectorizer(use_idf=True)\n",
    "                    tfIdf = tfIdfVectorizer.fit_transform(temp['clean'].values)\n",
    "                    words=display_scores(tfIdfVectorizer,tfIdf)\n",
    "                    words_df = pd.DataFrame(words,columns = ['word','tfidf_score'])\n",
    "                    words_df = words_df[words_df['tfidf_score']>=1] #Removing Scarse Words\n",
    "                    #temp.loc['clean'] = temp['clean'].str.split()\n",
    "                    #temp.loc['top_words']=temp['clean'].apply(lambda x: list(set(x).intersection(set(words_df['word'].values))))\n",
    "                    words_df.loc[:,'Product'] = prd\n",
    "                    words_df.loc[:,'Type'] =typ\n",
    "                    words_df.loc[:,'Industry'] =ind\n",
    "                    top_words = top_words.append(words_df)\n",
    "                except:\n",
    "                    pass\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_df = get_top_words(input_df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tfidf_score</th>\n",
       "      <th>Product</th>\n",
       "      <th>Type</th>\n",
       "      <th>Industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>holds</td>\n",
       "      <td>66.457184</td>\n",
       "      <td>Bike</td>\n",
       "      <td>Light Vehicles</td>\n",
       "      <td>Automobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stake</td>\n",
       "      <td>64.095917</td>\n",
       "      <td>Bike</td>\n",
       "      <td>Light Vehicles</td>\n",
       "      <td>Automobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>joint</td>\n",
       "      <td>46.088944</td>\n",
       "      <td>Bike</td>\n",
       "      <td>Light Vehicles</td>\n",
       "      <td>Automobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>venture</td>\n",
       "      <td>44.448137</td>\n",
       "      <td>Bike</td>\n",
       "      <td>Light Vehicles</td>\n",
       "      <td>Automobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>called</td>\n",
       "      <td>40.290874</td>\n",
       "      <td>Bike</td>\n",
       "      <td>Light Vehicles</td>\n",
       "      <td>Automobile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>young</td>\n",
       "      <td>1.013193</td>\n",
       "      <td>Hydro</td>\n",
       "      <td>green energy</td>\n",
       "      <td>Power</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>drops</td>\n",
       "      <td>1.011372</td>\n",
       "      <td>Hydro</td>\n",
       "      <td>green energy</td>\n",
       "      <td>Power</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>constructed</td>\n",
       "      <td>1.009113</td>\n",
       "      <td>Hydro</td>\n",
       "      <td>green energy</td>\n",
       "      <td>Power</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354</th>\n",
       "      <td>crust</td>\n",
       "      <td>1.007291</td>\n",
       "      <td>Hydro</td>\n",
       "      <td>green energy</td>\n",
       "      <td>Power</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1355</th>\n",
       "      <td>locations</td>\n",
       "      <td>1.005124</td>\n",
       "      <td>Hydro</td>\n",
       "      <td>green energy</td>\n",
       "      <td>Power</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4672 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  tfidf_score Product            Type    Industry\n",
       "0           holds    66.457184    Bike  Light Vehicles  Automobile\n",
       "1           stake    64.095917    Bike  Light Vehicles  Automobile\n",
       "2           joint    46.088944    Bike  Light Vehicles  Automobile\n",
       "3         venture    44.448137    Bike  Light Vehicles  Automobile\n",
       "4          called    40.290874    Bike  Light Vehicles  Automobile\n",
       "...           ...          ...     ...             ...         ...\n",
       "1351        young     1.013193   Hydro    green energy       Power\n",
       "1352        drops     1.011372   Hydro    green energy       Power\n",
       "1353  constructed     1.009113   Hydro    green energy       Power\n",
       "1354        crust     1.007291   Hydro    green energy       Power\n",
       "1355    locations     1.005124   Hydro    green energy       Power\n",
       "\n",
       "[4672 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           holds\n",
       "1           stake\n",
       "2           joint\n",
       "3         venture\n",
       "4          called\n",
       "          ...    \n",
       "1349       lesser\n",
       "1351        young\n",
       "1352        drops\n",
       "1354        crust\n",
       "1355    locations\n",
       "Name: word, Length: 3070, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words_df['word'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tfidf_score</th>\n",
       "      <th>Product</th>\n",
       "      <th>Type</th>\n",
       "      <th>Industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ford</td>\n",
       "      <td>21.792333</td>\n",
       "      <td>Bike</td>\n",
       "      <td>Light Vehicles</td>\n",
       "      <td>Automobile</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  tfidf_score Product            Type    Industry\n",
       "11  ford    21.792333    Bike  Light Vehicles  Automobile"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words_df[top_words_df['word']=='ford']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_df[top_words_df['word']=='system']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_df.groupby('word')['Industry'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_df.to_csv('top_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_words = {r'(\\b){}(\\b)'.format(k):r'\\1{}\\2'.format(v) for k,v in vocab_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df_processed['replaced_words'] = input_df_processed['clean'].replace(replace_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df_processed[y_columns].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training (Prepare Train & Validation Sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_valid = train_test_split(\n",
    " input_df_processed, test_size=0.1, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X training shape (21518, 3070) float64\n",
      "Y training shape (21518, 3) float64\n",
      "X validation shape (2391, 3070) float64\n",
      "Y validation shape (2391, 3) float64\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df= 1,\n",
    "                             #max_df =100,\n",
    "                             lowercase = True,\n",
    "                             vocabulary = top_words_df['word'].drop_duplicates(),\n",
    "                             #sublinear_tf = True,\n",
    "                             ngram_range=(1,1)\n",
    "                            ) \n",
    "vectorizer.fit(df_train['clean'].apply(lambda x: np.str_(x)))\n",
    "x_train = vectorizer.fit_transform(df_train['clean'].apply(lambda x: np.str_(x)))\n",
    "\n",
    "# we need the class labels encoded into integers for functions in the pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "oe = OrdinalEncoder()\n",
    "y_train = oe.fit_transform(df_train[y_columns].values.reshape(-1, 3))\n",
    "\n",
    "x_valid = vectorizer.transform(df_valid['clean'].apply(lambda x: np.str_(x)))\n",
    "y_valid = oe.transform(df_valid[y_columns].values.reshape(-1, 3))\n",
    "\n",
    "print('X training shape', x_train.shape, x_train.dtype)\n",
    "print('Y training shape', y_train.shape, y_train.dtype)\n",
    "print('X validation shape', x_valid.shape, x_valid.dtype)\n",
    "print('Y validation shape', y_valid.shape, y_valid.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf=ClassifierChain(SGDClassifier(random_state=1, loss='log',class_weight='balanced', n_jobs=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=ClassifierChain(RandomForestClassifier(n_estimators = 300,random_state=1,max_depth=20,class_weight='balanced', n_jobs=-1, oob_score = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_metric(y,y_pred, **kwargs):\n",
    "    \n",
    "    score1 = max(0,100*f1_score(y[:,2], y_pred[:,0], average='weighted'))\n",
    "    score2 = max(0,100*f1_score(y[:,0], y_pred[:,1], average='weighted'))\n",
    "    score3 = max(0,100*f1_score(y[:,1], y_pred[:,2], average='weighted'))\n",
    "    return (0.5*score1+0.3*score2+0.3*score3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierChain(base_estimator=SGDClassifier(class_weight='balanced',\n",
       "                                             loss='log', n_jobs=-1,\n",
       "                                             random_state=1))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probas = clf.predict_proba(x_valid)\n",
    "y_pred = clf.predict(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = pd.DataFrame(oe.inverse_transform(y_pred),columns = ['Industry','Type','Product'])\n",
    "#predictions['ID'] = df_valid['ID']\n",
    "predictions=predictions.reindex(columns=['ID','Industry','Type','Product'])\n",
    "predictions_probas = pd.DataFrame(y_pred_probas,columns = ['Industry','Type','Product'])\n",
    "#error_metric(Y_valid,Y_pred)\n",
    "predictions_probas['mean'] = (predictions_probas['Industry'] + predictions_probas['Type'] + predictions_probas['Product'])/3\n",
    "predictions_probas.to_csv('probas.csv')\n",
    "#predictions.to_csv('predictions.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(predictions_probas['mean'], bins = 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(predictions_probas['Industry'], bins = 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(predictions_probas['Type'], bins = 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_metric(y_valid,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
